{\rtf1\mac\ansicpg10000\cocoartf102
{\fonttbl\f0\fswiss\fcharset77 Arial-BoldMT;\f1\fswiss\fcharset77 ArialMT;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww19160\viewh14720\viewkind1\viewscale100
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural

\f0\b\fs20 \cf0 Big picture:
\f1\b0 \
\
See the second half of this for more details!\
\
1) Find the list of K-mers in the fragments.  This list should be non-redundant, e.g., no mer should be listed twice.  Remove mers that occur exactly once in the fragments.\
2) Construct a perfect hash function for the list of K-mers.\
3) Hash.  For each mer found in step 1, insert the sequence IID the mer belongs to into the hash table.  This will group related fragments together.\
4) Invert the table to get, for each fragment, the list of other fragmentss that have a mer in common with it.\
5) For each fragment, count mers in the set of related fragments to find the overlaps.\
6) Interface to the existing CA code base to construct overlaps.\
\

\f0\b Details:
\f1\b0 \
\

\f0\b (1)
\f1\b0   Use randomly generated hash functions to find the set of distinct K-mers (K ~= 100) in the sequence.  For i=0 to length of sequence, hash the mer that begins at position i in the sequence.  If we have seen that hash value before, we _probably_ have seen this mer already. If we haven't seen the hash value, we definitely have not seen this mer, and can mark this location as "the mer that starts here is distinct".  Repeat until we stop seeing new mers, or we run out of time.\
\
For 64M of sequence, with most of the mers distinct, a hash function with 128M positions finds most of the distinct mers in 8 rounds. Doubling the size of the hash uses twice the memory, and works nearly twice as fast.\
\
To remove unique mers, we can take all mers that were NOT marked as "distinct" and repeat the algorithm.\
\
Which is unexpectedly nice!  Our first pass at marking distinct mers doesn't need to be very precise, as all we're doing is removing as many uniquely occuring mers as possible.  The second pass needs to be more careful, but also has fewer mers in the set.\
\
The flow is then\
  o markDistinctMers in parallel.  Use the -1 switch to tell us that this is the first pass, and we should be marking mers if we haven't seen them before.\
  o mergeMarkings to take those results and construct a bit mask of the fragment sequence.\
  o markDistinctMers in parallel.  Use the -2 switch, which takes as an argument the bit mask from pass 1, to tell us that this is the second pass, and we should ignore mers masked in pass 1.\
  o mergeMarkings of those results.  This mask is now the list of mers that we want to use in future steps.\
\

\f0\b (2) 
\f1\b0  Construct a perfect (or nearly-perfect) hash function.  I have at least one paper that shows how to do this using a hash function "only" six times bigger than the number of elements.  So, if we have 3 billion distinct K-mers, we need 18 billion hash buckets.\
\
I note that Synamatix found 1.75 billion distinct and not unique 96-mers in the macaque (de-dye-blobbed) fragments.\
\
If the hash function is only nearly perfect, we'll get collisions that will result in false overlaps, which we'll discover and handle later on.  Performance of later steps suffers if there are too many collisions (where the value of "too many" is to be decided later).\
\
Early versions of this plan didn't bother finding the distinct mers, or finding a nearly-perfect hash function, and just blindly hoped that whatever hash function was implemented would have few collisions.\
\

\f0\b (3)
\f1\b0   Hash.  Only use mers found in step 1.  Construct a table T:\
  T[hash(mer)]  =  \{ list of iid's \}.\
\
A huge amount of space is saved here, because we're hashing only those mers that could seed an overlap.  Again picking on macaque, there are about 6 billion iid's that would need to be stored.\
\
At 26 bits (64 million IIDs) each, we need 15.6GB to store 6 billion IIDs, plus 34 bits * however big the hash table is - suppose 12 billion entries, so 51GB.\
\
Trivially parallelizable.  Segment T so that it fits into memory -- 128 jobs.  Explore using message passing -- each node reads a subset of the fragments, hashes, sends the result to the node that has that piece of T -- instead of having each node hash everything and throwing out 127/128ths of the work.\
\

\f0\b (4)
\f1\b0   Invert the table.  Two possibilities.\
\
(a) directly invert - allocate an array A of lists, one for each iid. Scan T in order, fill out the lists in A for the iid's in each bucket. E.g., if T[xxx] = \{ iid1, iid2, iid3 \}, then we insert iid2 and iid3 into A[iid1], and similarily process iid2 and iid3.\
\
(b) defer the list expansion until we process each iid - we still allocate an array of lists, but put the hash value ("xxx") into the list instead of the other iid's.  This assumes we have random access to the hash table!\
\
Algorithm (a) should work better if the overlaps are thick and we see many shared K-mers between two sequences as we only need to store iid's once; (b) should work better if there are lots of thin overlaps.  Or maybe a combination of the two.  Store the hash table computed in step 3 on disk -- removing buckets with no data to save space, relabeling buckets with an ordinal index instead of the hash value.  Allocate 128 million lists, two for each iid, one for (a) and one for (b).  Read a hash bucket.  If there are a few fragments in it, add those to the (a) list, else add the bucket id to the (b) list.\
\
We can store lists as a linked list using an integer index for the pointers.  For the (a) list, we store 26-bits for a fragment iid with a 33-bit index pointer (allowing up to 8 billion fragment iids to be stored in our linked list; macaque had 6 billion).  1GB of memory will then hold about 140 million list entries.\
\
Using our already allocated 128 nodes, each node gets a subset of fragments to invert, and should see 6 billion / 128 = 50 million iids.\
\
Thus, each node now reads the disk-based hash table.  If a bucket contains an iid the node is working on, it adds entries to either the (a) or (b) list as appropriate.\
\
A message passing implementation might work, and if so, saves from writing the hash table to disk.  E.g., every node scans it's table, remembering which ones have a globally known iid I.  After the scan, it packages those buckets and sends them to the correct node.  The global iid I is updated, and we repeat until all buckets have been communicated.\
\

\f0\b (5) and (6)
\f1\b0   The line is a little blurry between step 5 and 6.\
\
Count mers, construct K-mer overlaps.  For each iid, load the frags listed in A[iid], count K-mers in the set, apply whatever rules to pick out the fragments that overlap this iid.\
\
If a hash collision did occur in step 2, an extra fragment, say iid9, will be in our fragment set.  When we count K-mers, we'll get no K-mer in common between the current iid, call it iid0, and iid9.  By definition, there will also be another fragment like this, say iid10 (else, the K-mer in iid9 would be unique).  Later, when we process iid9, we'll find the correct iid10 overlap.\
\
The last step already partitioned the data by iid, so the mapping to our 128 nodes is pretty obvious.  Disk contention will be a big problem here.\

\f0\b Stage 1
\f1\b0 \
\
In practice, none of the steps are exact.  Our hash functions have collisions.  The effect of collisions is described.\
\

\f0\b Step 1)
\f1\b0  Mark distinct -- the output from here is one bit per base that indicates if the mer starting at that base is the first time we have seen that mer (called the mark file).\
\
A collision will result in the failure to mark the first instance of a distinct mer.  Thus, the output is MOST of the distinct mers.\
\
This needs to split the hash space so that it can fit in memory.  If our hash size is 40 bits, we need 2^37 bytes of storage (128GB), or we can split into 256 512MB jobs, that each use the same set of random hash functions.\
\
It will make multiple passes through the input fragments, updating a bit file in 1MB chunks.  The file should be located in node-private scratch space (see below).\
\
After all jobs are done, merge the results to get a single mark file. Trivial to do, just OR together all the results.  For 32M reads, a single result file will be 4GB.  With 256 jobs as recommended previously, this is a total of 1TB of intermediate data.\
\
Each job can write its 4GB results file into a node-private scratch space.\
\
Dros willi has 1,855,723,475 mers (output file of size 221MB), with only 503,832,481 mers marked.  bzip2 compressed it 96.8%, down to 7.5MB (7410282 bytes).  3.2% of 1TB is 35,184,372,088 bytes.\
\
Thus, we can simply compress the output, copy it to network disk, and decompress on merge.  This will make merging rather expensive.\
\
Run length encode instead?  Most of our distinct mers will be at the start of the sequence, which should compress well.  Merging is difficult.\
\

\f0\b Step 2)
\f1\b0  Mark distinct, ignore any mer marked in step 1 -- ignore all the single-copy mers, and the first instance of all multiple copy mers.  The output will be one bit per base, marking the second time we see a (non-unique) mer in the input.\
\
Collisions from step 1 are handled here.  Any mer not marked as distinct in step 1 will be in our input.  If this mer is actually unique, we will label it as non-unique here.  If this mer is not unique, no harm is done -- instead of marking the second instance, we will mark the first instance, and we don't care.  Thus, a collision in step 1 does not degrade the quality of the result here, but does degrade the performance.  We tried to remove all unique mers, but a few slipped through.\
\
A collision in this step, as in step 1, will fail to mark as distinct an instance of a mer that actually is distinct.  This will result in the loss of a potential overlap.  We can minimize the effect of this by increasing the hash space, and trying several different hash functions.\
\
Operationally, regarding partitioning and merging, this step is the same as step 1.\
\
The output is interpreted as a list of the mers that will be useful for constructing overlaps.\
\

\f0\b Step 3)
\f1\b0  Construct a hash/signature function for the mers marked in step 2.  Hash all mers in the input, mark any mer that is listed in the hash function.  The result of this will be to have all copies of non-unique mers marked -- the set of mers that would be useful for constructing overlaps.\
\
A collision will falsely include a unique mer as being useful for overlaps.  As with step 1, this does not degrade the quality, but does degrade the performance.\
\
To reduce collisions, we can partition the merspace based on, e.g., mapping the first 8 bases A,G->0, C,T->1, to generate an 8-bit partition number.  This should evenly reduce the merspace by a factor of 256.  For 32M frags, each node should see 134,217,728 mers.  At 32 bytes each mer, 4GB of mer data (so we do need to use some sort of signature function).\
\
A signature function of size 2^32 (4 billion possible signatures) would fit into 2^29 bytes = 512MB, and would be 2^5 = 32 times bigger than the number of mers.\
\
The expected number of collisions (I THINK - it behaves this way anyway) is\
\
  [ (n)(n-1)/2 ] / [ 2^32 ]   n = 2^27, the number of things we have\
\
which is, well, a lot bigger than I expected.  2^53/2^32 = 2^21 = 2 million collisions.\
\
Step 3 has a much larger hash space, but produces output in one pass -- the output file is not updated, it is just written.  Two passes through the input are needed, the first to build a hash/signature function, the second to report signature matches.  The output can be a list of merNumbers that need to be marked instead of an updated bit file.\
\
Finally, when all jobs finish, the marked file can be updated.\
\

\f0\b Summary of the impact of poor hash/signature functions:
\f1\b0 \
\
1) Slight degradation of performace in later steps, as single copy mers with no chance of discovering overlaps, are not filtered out. This can be solved by applying more and more different hash functions to identify more distinct mers.\
\
2) Potential loss of an overlap.  Solved as in step 1.\
\
3) We use the hash function as a signature function.  Iterating, as in steps 1 and 2, makes the problem worse - our problem is not one of missing mers, it is of including too many.  Thus, we want the signatures to be as specific as possible, but also as small as possible.  We cannot construct a "perfect" hash function here, instead we use a large hash space hoping to minimize collisions.\
\

\f0\b Stage 2
\f1\b0 \
\
The goal here is to construct clusters of fragments that are related by a mer.  A fragment may be listed in more than one cluster.  The next stage will invert the clusters to provide a list of related fragments for each fragment.\
\
Again, we can partition mers on the n-bit fingerprint as before.  If I/O is a problem, or we need to sort the mers in a partition, write each partition to disk.\
\
We only care about mers marked by step 1.\
\
32M fragments -> 32GB sequence -> about 3G distinct mers.  Macaque had about 6 to 9G of mer instances.  If in 256 partition, each node gets 12M mers, or 38M mer instances.  At 200 + 26 bits per mer, this would be about 1073MB mer data per node.\
\
We want to build a table of\
        T[mer] -> iid1, iid2, iid3, iid4\
\
Option 1: Load the mer data, sort, coalesce to get clusters.  Disk based bucket sort?\
\
Option 2: Construct a perfect hash function, use that for a cluster id.  Assuming 10% load in the hash function, this would be 380M entries.  The hash table maps a hashed mer to an offset (26 bits) into a fragment iid table, at 120M entries, this is 390GB.  The fragment iid table stores 38M 26 bit words, for 123MB data.  So, about 550MB data.\
\
Sorting is likely to be faster; constructing the perfect hash function can't be quick.\
\
Output is a list of cluster IIDs (we don't care about the mer anymore) and fragment IIDs in that cluster.  We should have 12M clusters, and 38M iids, so the output here should be about 200MB per node.  256 nodes would result in 51GB data total.\
\
An index of "cluster iid starts at position X" would also probably be useful.\
\

\f0\b Stage 3\

\f1\b0 \
Goal is to examine the clusters, and generate the list of fragments related to a specific fragment.  One or both of the following:\
\
frag1:  frag2, frag3, frag4 frag1:  cluster1, cluster2, cluster3\
\
If there is indeed 51GB total output from the previous stage, 64 nodes would partition the data into memory.  Each node has a range of iids it cares about, reads all 51GB of data, building the obvious structure.\
\
The output can't be bigger than the input, as redundant entries will be removed.\
}